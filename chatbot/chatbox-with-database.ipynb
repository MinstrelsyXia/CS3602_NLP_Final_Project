{"cells":[{"cell_type":"markdown","metadata":{},"source":["完了 https://qwen.readthedocs.io/zh-cn/latest/framework/Langchain.html"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-12-24T15:41:09.011583Z","iopub.status.busy":"2024-12-24T15:41:09.011244Z","iopub.status.idle":"2024-12-24T15:41:09.108237Z","shell.execute_reply":"2024-12-24T15:41:09.107556Z","shell.execute_reply.started":"2024-12-24T15:41:09.011556Z"},"trusted":true},"outputs":[],"source":["from langchain_community.document_loaders.csv_loader import CSVLoader\n","from langchain_community.document_loaders.text import TextLoader\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","# from langchain.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_chroma import Chroma\n","# from langchain.llms import Qwen\n","from langchain.prompts import ChatPromptTemplate\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-24T15:41:11.019762Z","iopub.status.busy":"2024-12-24T15:41:11.019441Z","iopub.status.idle":"2024-12-24T15:41:11.173836Z","shell.execute_reply":"2024-12-24T15:41:11.173207Z","shell.execute_reply.started":"2024-12-24T15:41:11.019737Z"},"trusted":true},"outputs":[],"source":["## 1 文档读取\n","# 读取csv，返回list\n","def load_csv(path):\n","    # 每条记录为一个元素\n","    # https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html\n","    loader = CSVLoader(\n","        file_path=path,\n","        encoding='utf-8',# 编码\n","        csv_args={\n","                'delimiter': ',',\n","                'quotechar': '\"',\n","                'fieldnames': ['Index', 'Height', 'Weight'] # CSV 文件应该包含这三个字段\n","                }\n","    )\n","    data = loader.load()\n","    return data \n","# 读取pdf，返回list\n","def load_pdf(path):\n","    # 是以每页为一个元素的\n","    loader = PyPDFLoader(path)\n","    pages = loader.load_and_split() #why\n","    return pages\n","def load_txt(path):\n","    # https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.text.TextLoader.html\n","    loader=TextLoader(file_path=path,encoding='utf-8',)\n","    data=loader.load()\n","    return data\n","    \n","## 2 文档切块\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=200, # 指定每个文本块的目标大小，这里设置为200个字符。\n","    chunk_overlap=50, # 指定文本块之间的重叠字符数，这里设置为50个字符。\n","    length_function=len, # 用于测量文本长度的函数，这里使用Python内置的`len`函数。\n","    is_separator_regex=False, # 指定`separators`中的分隔符是否应被视为正则表达式，这里设置为False，表示分隔符是字面字符。\n","    separators=[\"\\n\\n\",  \"\\n\",   \" \",    \".\",    \",\",     \"，\",  \"。\", ] # 定义用于分割文本的分隔符列表。\n",")\n","pages = load_txt(\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/chatbot/dataset/hlm.txt\")\n","# texts = text_splitter.split_documents([pages[0].page_content])\n","texts = text_splitter.split_documents(pages)\n","# type(texts)=list\n","# texts[int]={page_content=\"...\",metadata={'source': '/kaggle/input/the-dream-of-red-mansion/hlm.txt'}}\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-24T15:42:10.807389Z","iopub.status.busy":"2024-12-24T15:42:10.8071Z","iopub.status.idle":"2024-12-24T15:42:12.056885Z","shell.execute_reply":"2024-12-24T15:42:12.056193Z","shell.execute_reply.started":"2024-12-24T15:42:10.807367Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA available: True\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3781514/2166334147.py:14: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n","  hf = HuggingFaceBgeEmbeddings(\n","/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["## 3 向量化\n","# reference: https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.huggingface.HuggingFaceBgeEmbeddings.html\n","# model_name = \"BAAI/bge-large-en-v1.5\"\n","import torch\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","\n","model_name = \"BAAI/bge-large-zh\"\n","model_kwargs = {'device': 'cuda'}\n","encode_kwargs = {'normalize_embeddings': True} # 当向量都被规范化（归一化）后，它们的范数都是1。\n","\n","# model_name = \"BAAI/bge-large-en-v1.5\"\n","# model_kwargs = {'device': 'cuda'}\n","# encode_kwargs = {'normalize_embeddings': True}\n","hf = HuggingFaceBgeEmbeddings(\n","    model_name=model_name,\n","    model_kwargs=model_kwargs,\n","    encode_kwargs=encode_kwargs\n",")\n","# embedding = hf.embed_query(\"贾宝玉是谁\")\n","# print(embedding)\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-24T15:42:16.225638Z","iopub.status.busy":"2024-12-24T15:42:16.225336Z","iopub.status.idle":"2024-12-24T15:43:57.506501Z","shell.execute_reply":"2024-12-24T15:43:57.505578Z","shell.execute_reply.started":"2024-12-24T15:42:16.225614Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[".这政老爹的夫人王氏，头胎生的公子，名唤贾珠，十四岁进学，不到二十岁就娶了妻生了子，一病死了.第二胎生了一位小姐，生在大年初一，这就奇了，不想后来又生一位公子，说来更奇，一落胎胞，嘴里便衔下一块五彩晶莹的玉来，上面还有许多字迹，就取名叫作宝玉.你道是新奇异事不是？”\n"]}],"source":["## 4 创建向量数据库\n","# 快速创建数据库\n","# reference: https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#langchain_community.vectorstores.chroma.Chroma.from_documents\n","vectorstore = Chroma.from_documents(\n","   documents = texts, \n","   embedding = hf, #第三步得到的用于计算embedding的模型\n","   ids = None,\n","   collection_metadata = {\"hnsw:space\": \"cosine\"}, # 算余弦相似度\n","   )\n","# todo: 保存vectorstore\n","# 相似度方法通过查询文本检索数据 示例\n","query = \"贾宝玉是谁\"\n","docs = vectorstore.similarity_search(query)\n","print(docs[0].page_content)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-24T15:49:22.124555Z","iopub.status.busy":"2024-12-24T15:49:22.124263Z","iopub.status.idle":"2024-12-24T15:49:23.636666Z","shell.execute_reply":"2024-12-24T15:49:23.63603Z","shell.execute_reply.started":"2024-12-24T15:49:22.124533Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n"]}],"source":["\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import re\n","import os\n","# 设置可见的 GPU 设备为 cuda:0\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","def load_single_model(model_path,torch_dtype,trust_remote_code,device_map,use_cache):\n","    return AutoModelForCausalLM.from_pretrained(\n","            pretrained_model_name_or_path=model_path, \n","            torch_dtype=torch_dtype,\n","            trust_remote_code=trust_remote_code,  # Qwen模型需要这个参数\n","            device_map=device_map,  # 可选，用于自动处理模型加载到设备\n","            use_cache=use_cache\n","        )\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_path = \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/peft_3b/checkpoint-30000\"\n","max_position_embeddings = 4096 # 模型支持的最大长度\n","tokenizer = AutoTokenizer.from_pretrained(model_path,device_map=\"auto\" )\n","model = load_single_model(model_path,\"bfloat16\",True,\"auto\",False)\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 定义生成回复的函数\n","def generate_response(model, tokenizer, conversation_history, user_input):\n","    \"\"\"\n","    根据用户输入和对话历史生成模型回复。\n","    \"\"\"\n","    # 更新对话历史②\n","    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n","    print(user_input)\n","    \n","    text=tokenizer.apply_chat_template(conversation_history,tokenize=False,add_generation_prompt=True)\n","    inputs=tokenizer([text],return_tensors=\"pt\").to(model.device)\n","    if len(inputs[\"input_ids\"][0])>max_position_embeddings:\n","        # 不移除system 移除一轮对话\n","        conversation_history.pop(1) \n","        conversation_history.pop(2)\n","        text=tokenizer.apply_chat_template(conversation_history,tokenize=False,add_generation_prompt=True)\n","        inputs=tokenizer([text],return_tensors=\"pt\").to(model.device)\n","\n","    # print(text)查看输入的所有prompt\n","    \n","    outputs = model.generate(**inputs,pad_token_id=tokenizer.eos_token_id, #在生成时用eos填充序列\n","                            max_new_tokens=100, #新生成文本长度\n","                            # num_beams=5,\n","                            # temperature=0.7,\n","                            # top_k=50,\n","                            # top_p=0.95,\n","                            # repetition_penalty=1.2\n","                            )\n","    # print(outputs)\n","    response = tokenizer.decode(outputs[:, inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True) #在解码过程中跳过特殊符号如eos pad\n","    print(\"Bot:\",response)\n","    # 添加对话历史②\n","    conversation_history.pop()\n","    last_round_content=conversation_history[-1][\"content\"]\n","    match = re.search(r'\\[Round (\\d+)\\]', last_round_content)\n","    if match:\n","        last_round = int(match.group(1))\n","    else:\n","        last_round = 0\n","    conversation_history.append({\"role\": \"user\", \"content\": f\"[Round {last_round+1}]:{user_input}\"})\n","    conversation_history.append({\"role\": \"assistant\", \"content\": f\"[Round {last_round+1}]:{response.strip()}\"})\n","    return response.strip()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-12-24T15:55:43.534566Z","iopub.status.busy":"2024-12-24T15:55:43.53426Z","iopub.status.idle":"2024-12-24T15:55:58.40011Z","shell.execute_reply":"2024-12-24T15:55:58.399357Z","shell.execute_reply.started":"2024-12-24T15:55:43.534544Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Assistant:很抱歉，我无法回答这个问题。因为提示中没有提供足够的信息，我无法理解您的问题。如果您能提供更多的信息，我将尽力为您提供帮助。\n"]}],"source":["## 5\n","# 创建一个检索器 https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#langchain_community.vectorstores.chroma.Chroma.as_retriever\n","from langchain_core.output_parsers.string import StrOutputParser\n","from typing import Sequence\n","from langchain.schema import Document\n","from langchain.schema.document import Document\n","\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # k 表示要检索的结果数量\n","# query = \"萧炎是谁?\"\n","# https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html#langchain_core.vectorstores.base.VectorStoreRetriever\n","query = \"你好\"\n","docs = retriever.invoke(query)\n","\n","# 实例化自定义模型\n","llm = Qwen()\n","PROMPT_TEMPLATE = \"请根据提示回答问题\"\n","template = \"\"\"基于下列红楼梦的背景，回答问题。\n","红楼梦的背景：{context}\n","问题：{question}\n","\"\"\"\n","\n","def format_docs(docs: Sequence[Document]) -> str:\n","    formatted_docs = []\n","    for i, doc in enumerate(docs):\n","        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n","        formatted_docs.append(doc_string)\n","    return \"\\n\".join(formatted_docs)\n","prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n","# https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\n","\n","output_parser = StrOutputParser()\n","# 构建 chain\n","chain = prompt | llm | output_parser\n","res = chain.invoke(\n","                    {\n","                    \"context\": format_docs(docs),\n","                    \"question\": query\n","                    }\n","                    )\n","\n","print(res)\n","# 为什么《西游记》里孙悟空叫唐僧师父"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_variables=[] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='请根据提示回答问题'), additional_kwargs={})]\n"]}],"source":["print(prompt)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6367238,"sourceId":10288467,"sourceType":"datasetVersion"},{"modelId":164048,"modelInstanceId":141432,"sourceId":166218,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat":4,"nbformat_minor":4}
