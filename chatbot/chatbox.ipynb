{"cells":[{"cell_type":"markdown","metadata":{},"source":["# https://qwen.readthedocs.io/zh-cn/stable/inference/chat.html#"]},{"cell_type":"markdown","metadata":{},"source":["# 加载模型"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n"]}],"source":["# 修改实现 参考官方prompt格式\n","import argparse\n","from dataclasses import dataclass, field\n","from typing import Optional, List, Dict\n","import sys\n","import torch\n","from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n","import re\n","import os\n","# 设置可见的 GPU 设备为 cuda:0\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","def load_single_model(model_path,torch_dtype,trust_remote_code,device_map,use_cache):\n","    return AutoModelForCausalLM.from_pretrained(\n","            pretrained_model_name_or_path=model_path, \n","            torch_dtype=torch_dtype,\n","            trust_remote_code=trust_remote_code,  # Qwen模型需要这个参数\n","            device_map=device_map,  # 可选，用于自动处理模型加载到设备\n","            use_cache=use_cache\n","        )\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_path = \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/peft_3b/checkpoint-30000\"\n","max_position_embeddings = 4096 # 模型支持的最大长度\n","tokenizer = AutoTokenizer.from_pretrained(model_path,device_map=\"auto\" )\n","model = load_single_model(model_path,\"bfloat16\",True,\"auto\",False)\n","model = model.to(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 实现对话函数"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# 定义生成回复的函数\n","def generate_response(model, tokenizer, conversation_history, user_input):\n","    \"\"\"\n","    根据用户输入和对话历史生成模型回复。\n","    \"\"\"\n","    # 更新对话历史②\n","    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n","    print(user_input)\n","    \n","    text=tokenizer.apply_chat_template(conversation_history,tokenize=False,add_generation_prompt=True)\n","    inputs=tokenizer([text],return_tensors=\"pt\").to(model.device)\n","    if len(inputs[\"input_ids\"][0])>max_position_embeddings:\n","        # 不移除system 移除一轮对话\n","        conversation_history.pop(1) \n","        conversation_history.pop(2)\n","        text=tokenizer.apply_chat_template(conversation_history,tokenize=False,add_generation_prompt=True)\n","        inputs=tokenizer([text],return_tensors=\"pt\").to(model.device)\n","\n","    # print(text)查看输入的所有prompt\n","    \n","    outputs = model.generate(**inputs,pad_token_id=tokenizer.eos_token_id, #在生成时用eos填充序列\n","                            max_new_tokens=100, #新生成文本长度\n","                            # num_beams=5,\n","                            # temperature=0.7,\n","                            # top_k=50,\n","                            # top_p=0.95,\n","                            # repetition_penalty=1.2\n","                            )\n","    # print(outputs)\n","    response = tokenizer.decode(outputs[:, inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True) #在解码过程中跳过特殊符号如eos pad\n","    print(\"Bot:\",response)\n","    # 添加对话历史②\n","    conversation_history.pop()\n","    last_round_content=conversation_history[-1][\"content\"]\n","    match = re.search(r'\\[Round (\\d+)\\]', last_round_content)\n","    if match:\n","        last_round = int(match.group(1))\n","    else:\n","        last_round = 0\n","    conversation_history.append({\"role\": \"user\", \"content\": f\"[Round {last_round+1}]:{user_input}\"})\n","    conversation_history.append({\"role\": \"assistant\", \"content\": f\"[Round {last_round+1}]:{response.strip()}\"})\n","    return response.strip()\n","\n","# 定义主聊天逻辑\n","def chat(model, tokenizer):\n","    \"\"\"\n","    主聊天逻辑，支持 quit、newsession 等指令。\n","    \"\"\"\n","    global conversation_history\n","    conversation_history=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","    print(\"chat start\")\n","    while True:\n","        # 获取用户输入\n","        user_input = input(\"User: \").strip()\n","        # 处理不同指令 ③\n","        if user_input.lower() == \"\\quit\":\n","            print(\"Session ended. Bye!\")\n","            break\n","        elif user_input.lower() == \"\\\\newsession\":\n","            conversation_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n","            print(\"Conversation history cleaned.\")\n","        else:\n","            response = generate_response(model, tokenizer, conversation_history, user_input)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["chat start\n","hello\n","Bot: Hi there! How can I assist you today? Is there something specific you would like to know or do?\n","Session ended. Bye!\n"]}],"source":["chat(model, tokenizer)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6367238,"sourceId":10288467,"sourceType":"datasetVersion"},{"modelId":164048,"modelInstanceId":141432,"sourceId":166218,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat":4,"nbformat_minor":4}
