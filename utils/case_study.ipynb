{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:31:47.629916Z",
     "iopub.status.busy": "2024-12-24T10:31:47.629642Z",
     "iopub.status.idle": "2024-12-24T10:31:50.248612Z",
     "shell.execute_reply": "2024-12-24T10:31:50.247891Z",
     "shell.execute_reply.started": "2024-12-24T10:31:47.629891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "import os\n",
    "# set visible gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# 在需要打印的地方\n",
    "def pretty_print(text, width=80):\n",
    "    print(\"\\n\".join(textwrap.wrap(text, width=width)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:32:11.742623Z",
     "iopub.status.busy": "2024-12-24T10:32:11.741445Z",
     "iopub.status.idle": "2024-12-24T10:32:11.749542Z",
     "shell.execute_reply": "2024-12-24T10:32:11.748813Z",
     "shell.execute_reply.started": "2024-12-24T10:32:11.742569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here\n",
    "    max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"The max length of tokenized data.\"\n",
    "        }\n",
    "    )\n",
    "    skip_too_long: Optional[bool] = field(\n",
    "        default = False,\n",
    "        metadata = {\n",
    "            \"help\" : \"whether to skip those longer than max length of tokenized data\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:35:39.700030Z",
     "iopub.status.busy": "2024-12-24T10:35:39.699668Z",
     "iopub.status.idle": "2024-12-24T10:35:39.710538Z",
     "shell.execute_reply": "2024-12-24T10:35:39.709565Z",
     "shell.execute_reply.started": "2024-12-24T10:35:39.700000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def loading(plm_model_path,sft_model_path,sft_model_path_2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser = HfArgumentParser(dataclass_types=[ModelArguments, DataArguments, TrainingArguments])\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    dataset = datasets.load_dataset(path='csv', data_files=data_args.dataset_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_args.model_name_or_path)\n",
    "    \n",
    "    model_plm = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=plm_model_path, \n",
    "            torch_dtype=model_args.torch_dtype,\n",
    "            trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "            device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "            use_cache=False\n",
    "        )\n",
    "    model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=sft_model_path, \n",
    "            torch_dtype=model_args.torch_dtype,\n",
    "            trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "            device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "            use_cache=False\n",
    "        )\n",
    "    model_sft_2 = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=sft_model_path_2, \n",
    "            torch_dtype=model_args.torch_dtype,\n",
    "            trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "            device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "            use_cache=False\n",
    "        )       \n",
    "    return dataset,model_plm.to(device),model_sft.to(device),model_sft_2.to(device),tokenizer\n",
    "    \n",
    "def test(dataset,model_plm,model_sft,model_sft_2,tokenizer,your_input=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if type(your_input)==int:\n",
    "        sample = dataset['train'][your_input]\n",
    "        output_text = sample[\"output\"]\n",
    "        \n",
    "        text = \"instruction: \" + sample[\"instruction\"] if sample[\"instruction\"] else \"\"\n",
    "        text += \"\\n input: \" + sample[\"input\"] if sample[\"input\"] else \"\"\n",
    "        \n",
    "        print(\"Text: \", text)\n",
    "        print(\"GT: \", output_text)\n",
    "        print(\"===\")\n",
    "        # 不进行 padding，只进行截断\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    elif type(your_input)==str:\n",
    "        inputs=tokenizer(your_input, return_tensors=\"pt\")\n",
    "\n",
    "    \n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # # 查看生成的输入 IDs\n",
    "    # print(\"Input IDs:\", inputs['input_ids'])\n",
    "    \n",
    "    # # 查看生成的 attention_mask，不会被padding\n",
    "    # print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "    \n",
    "    # # 查看 pad_token_id\n",
    "    # print(\"Pad Token ID:\", tokenizer.pad_token_id)\n",
    "    inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    max_tokens = 256\n",
    "    generate_ids = model_plm.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    generated_text_plm = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    pretty_print(f\"plm:{generated_text_plm}\\n\")\n",
    "    print(\".....\")\n",
    "    generate_ids = model_sft.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    generated_text_sft = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    pretty_print(f\"sft:{generated_text_sft}\\n\")\n",
    "    print(\".....\")\n",
    "    generate_ids = model_sft_2.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    generated_text_sft_2 = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    pretty_print(f\"sft_2:{generated_text_sft_2}\\n\")\n",
    "    print(\"==========\")\n",
    "    return generated_text_plm,generated_text_sft,generated_text_sft_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:35:55.362514Z",
     "iopub.status.busy": "2024-12-24T10:35:55.362022Z",
     "iopub.status.idle": "2024-12-24T10:36:20.040323Z",
     "shell.execute_reply": "2024-12-24T10:36:20.039414Z",
     "shell.execute_reply.started": "2024-12-24T10:35:55.362463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    # \"--arg1\", \"value1\",\n",
    "    # \"--arg2\", \"value2\",\n",
    "    # ...\n",
    "    ### cjy\n",
    "    \"--model_name_or_path\", \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\",\n",
    "    \"--dataset_path\", \"/home/xiaxinyuan/.cache/kagglehub/datasets/thedevastator/alpaca-language-instruction-training/versions/2/train.csv\",\n",
    "    \"--torch_dtype\", \"bfloat16\", #see Qwen2.5-0.5B/config.json?\n",
    "    \"--output_dir\", \"output/1227/\", # --output_dir 参数在 TrainingArguments 中有\n",
    "    \"--remove_unused_columns\", \"False\", #ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [output, instruction, input]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`\n",
    "    \"--max_length\", \"512\",\n",
    "    ### xxy\n",
    "    \"--skip_too_long\",\"True\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--lr_scheduler_type\",\"cosine\",\n",
    "    \"--optim\", \"adamw_hf\",\n",
    "    \"--warmup_ratio\", \"0.03\",\n",
    "    \"--weight_decay\", \"0.003\",\n",
    "    ### xxy\n",
    "    ### cjy\n",
    "    \"--per_device_train_batch_size\", \"4\",  # 设置训练的 batch size\n",
    "    \"--per_device_eval_batch_size\", \"4\", \n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"3\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合 \n",
    "    \"--bf16\",\"True\",  # 开启混合精度加速\n",
    "]\n",
    "dataset,model_plm,model_sft,model_sft_2,tokenizer=loading(plm_model_path=\"/home/xiaxinyuan/.cache/kagglehub/models/qwen-lm/qwen2.5/transformers/0.5b/1/\", # 修改为你的模型路径\n",
    "                                    sft_model_path=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\",\n",
    "                                    sft_model_path_2=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/0.5B/checkpoint-50000\") # 修改为你的模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:37:13.603657Z",
     "iopub.status.busy": "2024-12-24T10:37:13.603006Z",
     "iopub.status.idle": "2024-12-24T10:37:50.778022Z",
     "shell.execute_reply": "2024-12-24T10:37:50.777092Z",
     "shell.execute_reply.started": "2024-12-24T10:37:13.603623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "instruction_set = [\n",
    "    \"college econometrics\\n \\\n",
    "    Which of the following statements concerning the regression population and sample is FALSE? \\\n",
    "    \\nA. The population is the total collection of all items of interest \\\n",
    "    \\nB. The population can be infinite \\\n",
    "    \\nC. In theory, the sample could be larger than the population \\\n",
    "    \\nD. A random sample is one where each individual item from the population is equally likely to be drawn. \\\n",
    "    \\nC. In theory, the sample could be larger than the population\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: There is a single choice question about college econometrics.\n",
      "Question:      Which of the following statements concerning the regression population and sample is FALSE?     . \n",
      "A. The population is the total collection of all items of interest\n",
      "B. The population can be infinite\n",
      "C. In theory, the sample could be larger than the population\n",
      "D. A random sample is one where each individual item from the population is equally likely to be drawn.\n",
      " Why the answer is C. In theory, the sample could be larger than the population? \n",
      " Think step by step. BOT: \n",
      "plm:1. The population is the total collection of all items of interest. 2. In\n",
      "theory, the sample could be larger than the population. 3. A random sample is\n",
      "one where each individual item from the population is equally likely to be\n",
      "drawn. 4. The population can be infinite. 5. The population is the total\n",
      "collection of all items of interest. 6. The population can be infinite. 7. In\n",
      "theory, the sample could be larger than the population. 8. A random sample is\n",
      "one where each individual item from the population is equally likely to be\n",
      "drawn. 9. A random sample is one where each individual item from the population\n",
      "is equally likely to be drawn. 10. The population is the total collection of all\n",
      "items of interest. 11. The population can be infinite. 12. In theory, the sample\n",
      "could be larger than the population. 13. A random sample is one where each\n",
      "individual item from the population is equally likely to be drawn. 14. The\n",
      "population can be infinite. 15. In theory, the sample could be larger than the\n",
      "population. 16. A random sample is one where each individual item from the\n",
      "population is equally likely to be drawn\n",
      ".....\n",
      "sft: In theory, the sample could be larger than the population. The population\n",
      "is finite, but the sample is usually taken from the population to represent it.\n",
      "So, the statement is true.\n",
      ".....\n",
      "sft_2: The sample is a subset of the population. So, the sample size could be\n",
      "smaller than the population size. So, statement C is false.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# instruction_set = [\n",
    "#     \"Which of the following statements concerning the regression population and sample is FALSE?\\n A. The population is the total collection of all items of interest\\n B. The population can be infinite\\n C. In theory, the sample could be larger than the population\\n D. A random sample is one where each individual item from the population is equally likely to be drawn.\"\n",
    "# ]\n",
    "import pandas as pd\n",
    "def get_template(instruction,choice,field,ans,mode = 'infer'):\n",
    "    template_set = [\n",
    "        # f\"HUMAN: There is a single choice question about {field}. \\nQ:{instruction}: {choice} \",\n",
    "        # f\"HUMAN: There is a single choice question about {field}. \\nQ:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college physics. Q:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college physics. \\nQ:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college mathematics. Q:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ:{instruction}: {choice}\\nLet's think step by step. A:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ:{instruction}: {choice}\\nLet's think step by step. BOT: A:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ: {instruction}:\\n{choice}\\nLet's think step by step. BOT:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ: {instruction}:\\n{choice}\\nA:\",\n",
    "        f\"HUMAN: There is a single choice question about {field}.\\nQuestion: {instruction}. \\n{choice}\\n Why the answer is {ans}? \\n Think step by step. BOT: \",\n",
    "    ]\n",
    "    return template_set\n",
    "for instruction in instruction_set: \n",
    "    ins = instruction.split(\"\\n\")\n",
    "    area = ins[0]\n",
    "    instruction = ins[1]\n",
    "    choice = ins[2:6]\n",
    "    ans=ins[6]\n",
    "    # print(area)\n",
    "    formatted_text = '\\n'.join(choice.strip() for choice in choice)\n",
    "    final_instructions = get_template(instruction,formatted_text,area,ans)\n",
    "    for final_instruction in final_instructions:\n",
    "        # pretty_print(final_instruction)\n",
    "        print(final_instruction)\n",
    "        plm_ans, sft1_ans, sft2_ans = test(dataset=dataset,\n",
    "            model_plm=model_plm,\n",
    "            model_sft=model_sft,\n",
    "            model_sft_2=model_sft_2,\n",
    "            tokenizer=tokenizer,\n",
    "            your_input=final_instruction) # 如果your_input是数字，则是被理解dataset中的index，即问alpaca中的第your_input个问题；如果是字符串，则是输入的文本\n",
    "        # save template set and answer in a row in excel\n",
    "        # df = pd.DataFrame({'template': [final_instruction], 'plm_ans': [plm_ans], 'sft1_ans': [sft1_ans], 'sft2_ans': [sft2_ans]})\n",
    "        # df.to_excel('case_study.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary source of the Sun’s energy is a series of thermonuclear reactions in which the energy produced is c^2 times the mass difference between A. two hydrogen atoms and one helium atom B. four hydrogen atoms and one helium atom C. six hydrogen atoms and two helium atoms D. three helium atoms and one carbon atom \n"
     ]
    }
   ],
   "source": [
    "for instruction in instruction_set: \n",
    "    ins = instruction.split(\"\\n\")\n",
    "    choices = ins[1:]\n",
    "    formatted_text = ' '.join(choice.strip() for choice in choices)\n",
    "    print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Write a humerous joke\n",
      "input: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"Write a humerous joke\"\n",
    "input_text = None\n",
    "print(f\"instruction: {instruction}\\ninput: {input_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sft_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/__init__.py:19: UserWarning: Starting from v0.4.0, all AMOTIC configuration files currently located in `./configs/datasets`, `./configs/models`, and `./configs/summarizers` will be migrated to the `opencompass/configs/` package. Please update your configuration file paths accordingly.\n",
      "  _warn_about_config_migration()\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MMluPPL' from 'opencompass.datasets' (/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceCausalLM\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     MMluPPL, \n\u001b[1;32m      4\u001b[0m     HellaswagCleanPPL, \n\u001b[1;32m      5\u001b[0m     WinograndePPL, \n\u001b[1;32m      6\u001b[0m     ARCePPL, \n\u001b[1;32m      7\u001b[0m     ARCcCleanPPL, \n\u001b[1;32m      8\u001b[0m     SuperGLUEBoolQPPL\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalRunner\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExampleSummarizer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MMluPPL' from 'opencompass.datasets' (/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from opencompass.models import HuggingFaceCausalLM\n",
    "from opencompass.datasets import (\n",
    "    MMluPPL, \n",
    "    HellaswagCleanPPL, \n",
    "    WinograndePPL, \n",
    "    ARCePPL, \n",
    "    ARCcCleanPPL, \n",
    "    SuperGLUEBoolQPPL\n",
    ")\n",
    "from opencompass.runners import LocalRunner\n",
    "from opencompass.summarizers import ExampleSummarizer\n",
    "\n",
    "from opencompass.models import HuggingFaceCausalLM\n",
    "from opencompass.datasets.mmlu import MMLU  # 修正导入路径\n",
    "from opencompass.datasets.hellaswag import HellaSwag\n",
    "from opencompass.datasets.winogrande import Winogrande\n",
    "from opencompass.datasets.arc import ARC_e, ARC_c\n",
    "from opencompass.runners import LocalRunner\n",
    "from opencompass.summarizers import ExampleSummarizer\n",
    "\n",
    "def train():\n",
    "    # 模型配置\n",
    "    model_cfg = dict(\n",
    "        type=HuggingFaceCausalLM,\n",
    "        path=\"output/0.5B/checkpoint-50000\",\n",
    "        tokenizer_kwargs={\n",
    "            'padding_side': 'left',\n",
    "            'truncation': 'left'\n",
    "        },\n",
    "        max_seq_len=2048,\n",
    "        batch_size=4,\n",
    "        model_kwargs={'device_map': 'auto'},\n",
    "    )\n",
    "\n",
    "    # 数据集配置\n",
    "    dataset_cfgs = [\n",
    "        dict(\n",
    "            type=MMLU,\n",
    "            path='mmlu',\n",
    "            name='mmlu',\n",
    "            ppl_eval=True  # 使用困惑度评估\n",
    "        ),\n",
    "        dict(\n",
    "            type=HellaSwag,\n",
    "            path='hellaswag',\n",
    "            name='hellaswag',\n",
    "            ppl_eval=True\n",
    "        ),\n",
    "        dict(\n",
    "            type=Winogrande,\n",
    "            path='winogrande',\n",
    "            name='winogrande',\n",
    "            ppl_eval=True\n",
    "        ),\n",
    "        dict(\n",
    "            type=ARC_e,\n",
    "            path='arc_e',\n",
    "            name='arc_e',\n",
    "            ppl_eval=True\n",
    "        ),\n",
    "        dict(\n",
    "            type=ARC_c,\n",
    "            path='arc_c',\n",
    "            name='arc_c',\n",
    "            ppl_eval=True\n",
    "        ),\n",
    "        dict(\n",
    "            type=BoolQ,\n",
    "            path='boolq',\n",
    "            name='boolq',\n",
    "            ppl_eval=True,\n",
    "            few_shot=True\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # 评测配置\n",
    "    eval_cfg = dict(\n",
    "        work_dir=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/evals/plm\",\n",
    "        summarizer=dict(type=ExampleSummarizer),\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # 创建评测实例\n",
    "    from opencompass import Evaluator\n",
    "    evaluator = Evaluator(\n",
    "        model_cfg=model_cfg,\n",
    "        dataset_cfgs=dataset_cfgs,\n",
    "        eval_cfg=eval_cfg\n",
    "    )\n",
    "\n",
    "    # 运行评测\n",
    "    results = evaluator.run()\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7143940,
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10583778,
     "datasetId": 6354526,
     "sourceId": 10282687,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10295729,
     "datasetId": 6173187,
     "sourceId": 10024541,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10158758,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
