{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:31:47.629916Z",
     "iopub.status.busy": "2024-12-24T10:31:47.629642Z",
     "iopub.status.idle": "2024-12-24T10:31:50.248612Z",
     "shell.execute_reply": "2024-12-24T10:31:50.247891Z",
     "shell.execute_reply.started": "2024-12-24T10:31:47.629891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "import os\n",
    "# set visible gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# 在需要打印的地方\n",
    "def pretty_print(text, width=80):\n",
    "    print(\"\\n\".join(textwrap.wrap(text, width=width)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:32:11.742623Z",
     "iopub.status.busy": "2024-12-24T10:32:11.741445Z",
     "iopub.status.idle": "2024-12-24T10:32:11.749542Z",
     "shell.execute_reply": "2024-12-24T10:32:11.748813Z",
     "shell.execute_reply.started": "2024-12-24T10:32:11.742569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here\n",
    "    max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"The max length of tokenized data.\"\n",
    "        }\n",
    "    )\n",
    "    skip_too_long: Optional[bool] = field(\n",
    "        default = False,\n",
    "        metadata = {\n",
    "            \"help\" : \"whether to skip those longer than max length of tokenized data\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:35:39.700030Z",
     "iopub.status.busy": "2024-12-24T10:35:39.699668Z",
     "iopub.status.idle": "2024-12-24T10:35:39.710538Z",
     "shell.execute_reply": "2024-12-24T10:35:39.709565Z",
     "shell.execute_reply.started": "2024-12-24T10:35:39.700000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_single_model(model_path,model_name,torch_dtype,trust_remote_code,device_map,use_cache):\n",
    "    return  AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_path, \n",
    "            torch_dtype=torch_dtype,\n",
    "            trust_remote_code=trust_remote_code,  # Qwen模型需要这个参数\n",
    "            device_map=None,  # 可选，用于自动处理模型加载到设备\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "def loading(model_list):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser = HfArgumentParser(dataclass_types=[ModelArguments, DataArguments, TrainingArguments])\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    dataset = datasets.load_dataset(path='csv', data_files=data_args.dataset_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_args.model_name_or_path)\n",
    "    for model_dict in model_list:\n",
    "        if model_dict['enable'] == True:\n",
    "            model_name = model_dict['name']\n",
    "            model_path = model_dict['path']\n",
    "            model = load_single_model(model_path,model_name,model_args.torch_dtype,True,\"auto\",False)\n",
    "            model = model.to(device)\n",
    "            # model_list[model_name] = model\n",
    "            model_dict['model'] = model\n",
    "    return dataset,model_list,tokenizer\n",
    "        \n",
    "    # model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    #         pretrained_model_name_or_path=sft_model_path, \n",
    "    #         torch_dtype=model_args.torch_dtype,\n",
    "    #         trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "    #         device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "    #         use_cache=False\n",
    "    #     )\n",
    "    # model_sft_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    #         pretrained_model_name_or_path=sft_model_path_2, \n",
    "    #         torch_dtype=model_args.torch_dtype,\n",
    "    #         trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "    #         device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "    #         use_cache=False\n",
    "    #     )       \n",
    "    # return dataset,model_plm.to(device),model_sft.to(device),model_sft_2.to(device),tokenizer\n",
    "\n",
    "def test_single_model(model,inputs,tokenizer,device):\n",
    "    \n",
    "    inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    max_tokens = 512\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    generate_ids = model.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    generated_text_plm = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return generated_text_plm\n",
    "\n",
    "def test(dataset,model_list,tokenizer,your_input=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if type(your_input)==int:\n",
    "        sample = dataset['train'][your_input]\n",
    "        output_text = sample[\"output\"]\n",
    "        \n",
    "        text = \"instruction: \" + sample[\"instruction\"] if sample[\"instruction\"] else \"\"\n",
    "        text += \"\\n input: \" + sample[\"input\"] if sample[\"input\"] else \"\"\n",
    "        \n",
    "        print(\"Text: \", text)\n",
    "        print(\"GT: \", output_text)\n",
    "        print(\"===\")\n",
    "        # 不进行 padding，只进行截断\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    elif type(your_input)==str:\n",
    "        inputs=tokenizer(your_input, return_tensors=\"pt\")\n",
    "    \n",
    "    # # 查看生成的输入 IDs\n",
    "    # print(\"Input IDs:\", inputs['input_ids'])\n",
    "    \n",
    "    # # 查看生成的 attention_mask，不会被padding\n",
    "    # print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "    \n",
    "    # # 查看 pad_token_id\n",
    "    # print(\"Pad Token ID:\", tokenizer.pad_token_id)\n",
    "\n",
    "    for model_dict in model_list:\n",
    "        if model_dict['enable'] == True:\n",
    "            model_name = model_dict['name']\n",
    "            model = model_dict['model']\n",
    "            generated_text = test_single_model(model,inputs,tokenizer,device)\n",
    "            print(f\"model:{model_name} done\")\n",
    "            pretty_print(f\"generated_text:{generated_text}\")\n",
    "            print(\".....\")\n",
    "\n",
    "    \n",
    "    # generate_ids = model_plm.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    # generated_text_plm = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    # pretty_print(f\"plm:{generated_text_plm}\\n\")\n",
    "    # print(\".....\")\n",
    "    # generate_ids = model_sft.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    # generated_text_sft = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    # pretty_print(f\"sft:{generated_text_sft}\\n\")\n",
    "    # print(\".....\")\n",
    "\n",
    "    # if model_sft_2 is not None:\n",
    "    #     generate_ids = model_sft_2.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    #     generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    #     generated_text_sft_2 = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    #     pretty_print(f\"sft_2:{generated_text_sft_2}\\n\")\n",
    "    #     print(\"==========\")\n",
    "    # return generated_text_plm,generated_text_sft,generated_text_sft_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    {\n",
    "        'name':'sft_p',\n",
    "        'path':'/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/sft_0.5b_p/checkpoint-50000',\n",
    "        'enable':False\n",
    "    },\n",
    "    {\n",
    "        'name':'sft_a',\n",
    "        'path':'/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/sft_0.5b_a/checkpoint-50000',\n",
    "        'enable':False\n",
    "    },\n",
    "    {\n",
    "        'name':'peft',\n",
    "        'path':'/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/peft_3b/checkpoint-30000',\n",
    "        'enable':True\n",
    "    },\n",
    "    {\n",
    "        'name': '3b-ins',\n",
    "        'path': \"/home/xiaxinyuan/.cache/kagglehub/models/qwen-lm/qwen2.5/transformers/3b-instruct/1\",\n",
    "        'enable':False\n",
    "    },\n",
    "    {\n",
    "        'name': '3b',\n",
    "        'path':\"/home/xiaxinyuan/.cache/kagglehub/models/qwen-lm/qwen2.5/transformers/3b/1/\",\n",
    "        'enable': False\n",
    "\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: If A = \\{1, 2, 3\\} then relation S = \\{\\(1, 1\\), \\(2, 2\\)\\} is \n",
      " A. symmetric only \n",
      " B. anti-symmetric only \n",
      " C. both symmetric and anti-symmetric \n",
      " D. an equivalence relation}\n",
      "input: None\n",
      "model:peft done\n",
      "generated_text: To determine the nature of the relation \\( S = \\{(1, 1), (2,\n",
      "2)\\} \\) on the set \\( A = \\{1, 2, 3\\} \\), we need to check if it satisfies the\n",
      "properties of symmetry and anti-symmetry.  1. **Symmetry**: A relation \\( S \\)\n",
      "on a set \\( A \\) is symmetric if for every pair \\( (a, b) \\in S \\), the pair \\(\n",
      "(b, a) \\) is also in \\( S \\). In this case, the only pair in \\( S \\) is \\( (1,\n",
      "1) \\). Since there is no \\( b \\) such that \\( (1, b) \\) is in \\( S \\), the\n",
      "relation \\( S \\) is symmetric.  2. **Anti-symmetry**: A relation \\( S \\) on a\n",
      "set \\( A \\) is anti-symmetric if for every pair \\( (a, b) \\in S \\) and \\( (b, a)\n",
      "\\in S \\), it must be that \\( a = b \\). In this case, the only pair in \\( S \\) is\n",
      "\\( (1, 1) \\). Since there is no \\( b \\) such that \\( (1, b) \\) is in \\( S \\),\n",
      "the relation \\( S \\) is anti-symmetric.  Since \\( S \\) is both symmetric and\n",
      "anti-symmetric, the correct answer is C. Both symmetric and anti-symmetric.\n",
      "Let's confirm this with Python code. ```python # Define the set A and the\n",
      "relation S A = {1, 2, 3} S = {(1, 1), (2, 2)}  # Function to check if a relation\n",
      "is symmetric def is_symmetric(S):     for a in S:         for b in S:\n",
      "if (a, b) in S and (b, a) not in S:                 return False     return True\n",
      "# Function to check if a relation is anti-symmetric def is_anti_symmetric(S):\n",
      "for a in S:         for b in S:             if (a, b) in S and (b, a) in S and a\n",
      "!= b:                 return False     return True  # Check if the relation S is\n",
      "symmetric and anti-symmetric is_symmetric_S = is_symmetric(S)\n",
      "is_anti_symmetric_S = is_anti_sym\n",
      ".....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    # \"--arg1\", \"value1\",\n",
    "    # \"--arg2\", \"value2\",\n",
    "    # ...\n",
    "    ### cjy\n",
    "    \"--model_name_or_path\", \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/peft_3b/checkpoint-30000\",\n",
    "    \"--dataset_path\", \"/home/xiaxinyuan/.cache/kagglehub/datasets/thedevastator/alpaca-language-instruction-training/versions/2/train.csv\",\n",
    "    \"--torch_dtype\", \"bfloat16\", #see Qwen2.5-0.5B/config.json?\n",
    "    \"--output_dir\", \"output/1227/\", # --output_dir 参数在 TrainingArguments 中有\n",
    "    \"--remove_unused_columns\", \"False\", #ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [output, instruction, input]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`\n",
    "    \"--max_length\", \"512\",\n",
    "    ### xxy\n",
    "    \"--skip_too_long\",\"True\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--lr_scheduler_type\",\"cosine\",\n",
    "    \"--optim\", \"adamw_hf\",\n",
    "    \"--warmup_ratio\", \"0.03\",\n",
    "    \"--weight_decay\", \"0.003\",\n",
    "    ### xxy\n",
    "    ### cjy\n",
    "    \"--per_device_train_batch_size\", \"4\",  # 设置训练的 batch size\n",
    "    \"--per_device_eval_batch_size\", \"4\", \n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"3\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合 \n",
    "    \"--bf16\",\"True\",  # 开启混合精度加速\n",
    "]\n",
    "dataset,model_list,tokenizer = loading(model_list)\n",
    "\n",
    "instruction = \"If A = \\{1, 2, 3\\} then relation S = \\{\\(1, 1\\), \\(2, 2\\)\\} is \\n A. symmetric only \\n B. anti-symmetric only \\n C. both symmetric and anti-symmetric \\n D. an equivalence relation}\"\n",
    "input_text = None\n",
    "print(f\"instruction: {instruction}\\ninput: {input_text}\")\n",
    "test(dataset,model_list,tokenizer,your_input=instruction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:35:55.362514Z",
     "iopub.status.busy": "2024-12-24T10:35:55.362022Z",
     "iopub.status.idle": "2024-12-24T10:36:20.040323Z",
     "shell.execute_reply": "2024-12-24T10:36:20.039414Z",
     "shell.execute_reply.started": "2024-12-24T10:35:55.362463Z"
    },
    "trusted": true
   },
   "source": [
    "# Past API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loading() got an unexpected keyword argument 'plm_model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pass your training arguments.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebook\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# \"--arg1\", \"value1\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bf16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 开启混合精度加速\u001b[39;00m\n\u001b[1;32m     30\u001b[0m ]\n\u001b[0;32m---> 31\u001b[0m dataset,model_plm,model_sft,model_sft_2,tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mloading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplm_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/xiaxinyuan/.cache/kagglehub/models/qwen-lm/qwen2.5/transformers/0.5b/1/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 修改为你的模型路径\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msft_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msft_model_path_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/0.5B/checkpoint-50000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 修改为你的模型路径\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: loading() got an unexpected keyword argument 'plm_model_path'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    # \"--arg1\", \"value1\",\n",
    "    # \"--arg2\", \"value2\",\n",
    "    # ...\n",
    "    ### cjy\n",
    "    \"--model_name_or_path\", \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/peft_3b/checkpoint-30000\",\n",
    "    \"--dataset_path\", \"/home/xiaxinyuan/.cache/kagglehub/datasets/thedevastator/alpaca-language-instruction-training/versions/2/train.csv\",\n",
    "    \"--torch_dtype\", \"bfloat16\", #see Qwen2.5-0.5B/config.json?\n",
    "    \"--output_dir\", \"output/1227/\", # --output_dir 参数在 TrainingArguments 中有\n",
    "    \"--remove_unused_columns\", \"False\", #ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [output, instruction, input]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`\n",
    "    \"--max_length\", \"512\",\n",
    "    ### xxy\n",
    "    \"--skip_too_long\",\"True\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--lr_scheduler_type\",\"cosine\",\n",
    "    \"--optim\", \"adamw_hf\",\n",
    "    \"--warmup_ratio\", \"0.03\",\n",
    "    \"--weight_decay\", \"0.003\",\n",
    "    ### xxy\n",
    "    ### cjy\n",
    "    \"--per_device_train_batch_size\", \"4\",  # 设置训练的 batch size\n",
    "    \"--per_device_eval_batch_size\", \"4\", \n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"3\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合 \n",
    "    \"--bf16\",\"True\",  # 开启混合精度加速\n",
    "]\n",
    "dataset,model_plm,model_sft,model_sft_2,tokenizer=loading(plm_model_path=\"/home/xiaxinyuan/.cache/kagglehub/models/qwen-lm/qwen2.5/transformers/0.5b/1/\", # 修改为你的模型路径\n",
    "                                    sft_model_path=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\",\n",
    "                                    sft_model_path_2=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/0.5B/checkpoint-50000\") # 修改为你的模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:37:13.603657Z",
     "iopub.status.busy": "2024-12-24T10:37:13.603006Z",
     "iopub.status.idle": "2024-12-24T10:37:50.778022Z",
     "shell.execute_reply": "2024-12-24T10:37:50.777092Z",
     "shell.execute_reply.started": "2024-12-24T10:37:13.603623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "instruction_set = [\n",
    "    \"college econometrics\\n \\\n",
    "    Which of the following statements concerning the regression population and sample is FALSE? \\\n",
    "    \\nA. The population is the total collection of all items of interest \\\n",
    "    \\nB. The population can be infinite \\\n",
    "    \\nC. In theory, the sample could be larger than the population \\\n",
    "    \\nD. A random sample is one where each individual item from the population is equally likely to be drawn. \\\n",
    "    \\nC. In theory, the sample could be larger than the population\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: There is a single choice question about college econometrics.\n",
      "Question:      Which of the following statements concerning the regression population and sample is FALSE?     . \n",
      "A. The population is the total collection of all items of interest\n",
      "B. The population can be infinite\n",
      "C. In theory, the sample could be larger than the population\n",
      "D. A random sample is one where each individual item from the population is equally likely to be drawn.\n",
      " Why the answer is C. In theory, the sample could be larger than the population? \n",
      " Think step by step. BOT: \n",
      "plm:1. The population is the total collection of all items of interest. 2. In\n",
      "theory, the sample could be larger than the population. 3. A random sample is\n",
      "one where each individual item from the population is equally likely to be\n",
      "drawn. 4. The population can be infinite. 5. The population is the total\n",
      "collection of all items of interest. 6. The population can be infinite. 7. In\n",
      "theory, the sample could be larger than the population. 8. A random sample is\n",
      "one where each individual item from the population is equally likely to be\n",
      "drawn. 9. A random sample is one where each individual item from the population\n",
      "is equally likely to be drawn. 10. The population is the total collection of all\n",
      "items of interest. 11. The population can be infinite. 12. In theory, the sample\n",
      "could be larger than the population. 13. A random sample is one where each\n",
      "individual item from the population is equally likely to be drawn. 14. The\n",
      "population can be infinite. 15. In theory, the sample could be larger than the\n",
      "population. 16. A random sample is one where each individual item from the\n",
      "population is equally likely to be drawn\n",
      ".....\n",
      "sft: In theory, the sample could be larger than the population. The population\n",
      "is finite, but the sample is usually taken from the population to represent it.\n",
      "So, the statement is true.\n",
      ".....\n",
      "sft_2: The sample is a subset of the population. So, the sample size could be\n",
      "smaller than the population size. So, statement C is false.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# instruction_set = [\n",
    "#     \"Which of the following statements concerning the regression population and sample is FALSE?\\n A. The population is the total collection of all items of interest\\n B. The population can be infinite\\n C. In theory, the sample could be larger than the population\\n D. A random sample is one where each individual item from the population is equally likely to be drawn.\"\n",
    "# ]\n",
    "import pandas as pd\n",
    "def get_template(instruction,choice,field,ans,mode = 'infer'):\n",
    "    template_set = [\n",
    "        # f\"HUMAN: There is a single choice question about {field}. \\nQ:{instruction}: {choice} \",\n",
    "        # f\"HUMAN: There is a single choice question about {field}. \\nQ:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college physics. Q:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college physics. \\nQ:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college mathematics. Q:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ:{instruction}: {choice}\\nLet's think step by step. A:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ:{instruction}: {choice}\\nLet's think step by step. BOT: A:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ: {instruction}:\\n{choice}\\nLet's think step by step. BOT:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ: {instruction}:\\n{choice}\\nA:\",\n",
    "        f\"HUMAN: There is a single choice question about {field}.\\nQuestion: {instruction}. \\n{choice}\\n Why the answer is {ans}? \\n Think step by step. BOT: \",\n",
    "    ]\n",
    "    return template_set\n",
    "for instruction in instruction_set: \n",
    "    ins = instruction.split(\"\\n\")\n",
    "    area = ins[0]\n",
    "    instruction = ins[1]\n",
    "    choice = ins[2:6]\n",
    "    ans=ins[6]\n",
    "    # print(area)\n",
    "    formatted_text = '\\n'.join(choice.strip() for choice in choice)\n",
    "    final_instructions = get_template(instruction,formatted_text,area,ans)\n",
    "    for final_instruction in final_instructions:\n",
    "        # pretty_print(final_instruction)\n",
    "        print(final_instruction)\n",
    "        plm_ans, sft1_ans, sft2_ans = test(dataset=dataset,\n",
    "            model_plm=model_plm,\n",
    "            model_sft=model_sft,\n",
    "            model_sft_2=model_sft_2,\n",
    "            tokenizer=tokenizer,\n",
    "            your_input=final_instruction) # 如果your_input是数字，则是被理解dataset中的index，即问alpaca中的第your_input个问题；如果是字符串，则是输入的文本\n",
    "        # save template set and answer in a row in excel\n",
    "        # df = pd.DataFrame({'template': [final_instruction], 'plm_ans': [plm_ans], 'sft1_ans': [sft1_ans], 'sft2_ans': [sft2_ans]})\n",
    "        # df.to_excel('case_study.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary source of the Sun’s energy is a series of thermonuclear reactions in which the energy produced is c^2 times the mass difference between A. two hydrogen atoms and one helium atom B. four hydrogen atoms and one helium atom C. six hydrogen atoms and two helium atoms D. three helium atoms and one carbon atom \n"
     ]
    }
   ],
   "source": [
    "for instruction in instruction_set: \n",
    "    ins = instruction.split(\"\\n\")\n",
    "    choices = ins[1:]\n",
    "    formatted_text = ' '.join(choice.strip() for choice in choices)\n",
    "    print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Write a humerous joke\n",
      "input: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"Write a humerous joke\"\n",
    "input_text = None\n",
    "print(f\"instruction: {instruction}\\ninput: {input_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction = \"Write a humerous joke\"\n",
    "model_list = [\n",
    "    {\n",
    "        'name':'peft',\n",
    "        'model':model_plm\n",
    "    },\n",
    "    {\n",
    "        'name':'sft',\n",
    "        'model':model_sft\n",
    "    }\n",
    "]\n",
    "dataset,model_list,tokenizer = loading(model_list,data_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/__init__.py:19: UserWarning: Starting from v0.4.0, all AMOTIC configuration files currently located in `./configs/datasets`, `./configs/models`, and `./configs/summarizers` will be migrated to the `opencompass/configs/` package. Please update your configuration file paths accordingly.\n",
      "  _warn_about_config_migration()\n",
      "/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TruthfulQA' from 'opencompass.datasets' (/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceCausalLM\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     TruthfulQA,\n\u001b[1;32m      4\u001b[0m     CivilComments,\n\u001b[1;32m      5\u001b[0m     HelmHarm,\n\u001b[1;32m      6\u001b[0m     BBQPPl,  \u001b[38;5;66;03m# 偏见评估\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     ToxiGen,  \u001b[38;5;66;03m# 有毒内容生成评估\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalRunner\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExampleSummarizer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TruthfulQA' from 'opencompass.datasets' (/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from opencompass.models import HuggingFaceCausalLM\n",
    "from opencompass.datasets import (\n",
    "    TruthfulQADataset,\n",
    "    CivilCommentsDataset,\n",
    "    CrowspairsDatasetV2,\n",
    "    CrowspairsDatasetCN,\n",
    "    CValuesDataset,\n",
    "    JigsawMultilingualDataset,  # 有毒内容生成评估\n",
    ")\n",
    "from opencompass.runners import LocalRunner\n",
    "from opencompass.summarizers import ExampleSummarizer\n",
    "\n",
    "def evaluate_safety():\n",
    "    # 模型配置保持不变\n",
    "    model_cfg = dict(\n",
    "        type=HuggingFaceCausalLM,\n",
    "        path=\"output/0.5B/checkpoint-50000\",\n",
    "        tokenizer_kwargs={\n",
    "            'padding_side': 'left',\n",
    "            'truncation': 'left'\n",
    "        },\n",
    "        max_seq_len=2048,\n",
    "        batch_size=4,\n",
    "        model_kwargs={'device_map': 'auto'},\n",
    "    )\n",
    "\n",
    "    # 修改为安全相关的数据集配置\n",
    "    dataset_cfgs = [\n",
    "        dict(\n",
    "            type=TruthfulQA,\n",
    "            path='truthfulqa',\n",
    "            name='truthfulqa',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型回答的真实性\n",
    "        ),\n",
    "        dict(\n",
    "            type=CivilComments,\n",
    "            path='civil_comments',\n",
    "            name='civil_comments',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型对有毒内容的处理\n",
    "        ),\n",
    "        dict(\n",
    "            type=HelmHarm,\n",
    "            path='helm/harm',\n",
    "            name='helm_harm',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型的安全性\n",
    "        ),\n",
    "        dict(\n",
    "            type=BBQPPl,\n",
    "            path='bbq',\n",
    "            name='bbq',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型的偏见\n",
    "        ),\n",
    "        dict(\n",
    "            type=ToxiGen,\n",
    "            path='toxigen',\n",
    "            name='toxigen',\n",
    "            ppl_eval=True,\n",
    "            # 评估有毒内容生成\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # 修改评测输出路径\n",
    "    eval_cfg = dict(\n",
    "        work_dir=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/evals/safety\",\n",
    "        summarizer=dict(type=ExampleSummarizer),\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # 创建评测实例\n",
    "    from opencompass import Evaluator\n",
    "    evaluator = Evaluator(\n",
    "        model_cfg=model_cfg,\n",
    "        dataset_cfgs=dataset_cfgs,\n",
    "        eval_cfg=eval_cfg\n",
    "    )\n",
    "\n",
    "    # 运行评测\n",
    "    results = evaluator.run()\n",
    "    return results\n",
    "# 运行评测\n",
    "results = evaluate_safety()\n",
    "\n",
    "# 查看具体数据集的结果\n",
    "print(\"TruthfulQA结果:\", results['truthfulqa'])\n",
    "print(\"有毒内容处理结果:\", results['civil_comments'])\n",
    "print(\"安全性评估结果:\", results['helm_harm'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7143940,
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10583778,
     "datasetId": 6354526,
     "sourceId": 10282687,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10295729,
     "datasetId": 6173187,
     "sourceId": 10024541,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10158758,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
