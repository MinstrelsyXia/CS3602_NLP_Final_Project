{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装opencompass：Kaggle上已经为我们准备好了其他常用包，只需安装opencompass用于评测即可。如果不在Kaggle上运行，则还需要安装其他必要包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:31:47.629916Z",
     "iopub.status.busy": "2024-12-24T10:31:47.629642Z",
     "iopub.status.idle": "2024-12-24T10:31:50.248612Z",
     "shell.execute_reply": "2024-12-24T10:31:50.247891Z",
     "shell.execute_reply.started": "2024-12-24T10:31:47.629891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import sys\n",
    "import torch\n",
    "from transformers import TrainingArguments, HfArgumentParser, Trainer, AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "import os\n",
    "# set visible gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# 在需要打印的地方\n",
    "def pretty_print(text, width=80):\n",
    "    print(\"\\n\".join(textwrap.wrap(text, width=width)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:32:11.742623Z",
     "iopub.status.busy": "2024-12-24T10:32:11.741445Z",
     "iopub.status.idle": "2024-12-24T10:32:11.749542Z",
     "shell.execute_reply": "2024-12-24T10:32:11.748813Z",
     "shell.execute_reply.started": "2024-12-24T10:32:11.742569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the arguments required for the main program.\n",
    "# NOTE: You can customize any arguments you need to pass in.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments for model\n",
    "    \"\"\"\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the LLM to fine-tune or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype.\"\n",
    "            ),\n",
    "            \"choices\": [\"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    # TODO: add your model arguments here\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments for data\n",
    "    \"\"\"\n",
    "    dataset_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The path to the fine-tuning dataset or its name on the Hugging Face Hub.\"\n",
    "        }\n",
    "    )\n",
    "    # TODO: add your data arguments here\n",
    "    max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"The max length of tokenized data.\"\n",
    "        }\n",
    "    )\n",
    "    skip_too_long: Optional[bool] = field(\n",
    "        default = False,\n",
    "        metadata = {\n",
    "            \"help\" : \"whether to skip those longer than max length of tokenized data\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:35:39.700030Z",
     "iopub.status.busy": "2024-12-24T10:35:39.699668Z",
     "iopub.status.idle": "2024-12-24T10:35:39.710538Z",
     "shell.execute_reply": "2024-12-24T10:35:39.709565Z",
     "shell.execute_reply.started": "2024-12-24T10:35:39.700000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_single_model(model_path,model_name,torch_dtype,trust_remote_code,device_map,use_cache):\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_path, \n",
    "            torch_dtype=torch_dtype,\n",
    "            trust_remote_code=trust_remote_code,  # Qwen模型需要这个参数\n",
    "            device_map=device_map,  # 可选，用于自动处理模型加载到设备\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "def loading(model_list):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    parser = HfArgumentParser(dataclass_types=[ModelArguments, DataArguments, TrainingArguments])\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    dataset = datasets.load_dataset(path='csv', data_files=data_args.dataset_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_args.model_name_or_path)\n",
    "    for model_dict in model_list:\n",
    "        if model_dict['enable'] == True:\n",
    "            model_name = model_dict['name']\n",
    "            model_path = model_dict['path']\n",
    "            model = load_single_model(model_path,model_name,model_args.torch_dtype,True,\"auto\",False)\n",
    "            model = model.to(device)\n",
    "            # model_list[model_name] = model\n",
    "            model_dict['model'] = model\n",
    "    return dataset,model_list,tokenizer\n",
    "        \n",
    "    # model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    #         pretrained_model_name_or_path=sft_model_path, \n",
    "    #         torch_dtype=model_args.torch_dtype,\n",
    "    #         trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "    #         device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "    #         use_cache=False\n",
    "    #     )\n",
    "    # model_sft_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    #         pretrained_model_name_or_path=sft_model_path_2, \n",
    "    #         torch_dtype=model_args.torch_dtype,\n",
    "    #         trust_remote_code=True,  # Qwen模型需要这个参数\n",
    "    #         device_map=\"auto\",  # 可选，用于自动处理模型加载到设备\n",
    "    #         use_cache=False\n",
    "    #     )       \n",
    "    # return dataset,model_plm.to(device),model_sft.to(device),model_sft_2.to(device),tokenizer\n",
    "\n",
    "def test_single_model(model,inputs,tokenizer,device):\n",
    "    \n",
    "    inputs['input_ids'] = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    max_tokens = 256\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    generate_ids = model.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    generated_text_plm = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return generated_text_plm\n",
    "\n",
    "def test(dataset,model_list,tokenizer,your_input=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if type(your_input)==int:\n",
    "        sample = dataset['train'][your_input]\n",
    "        output_text = sample[\"output\"]\n",
    "        \n",
    "        text = \"instruction: \" + sample[\"instruction\"] if sample[\"instruction\"] else \"\"\n",
    "        text += \"\\n input: \" + sample[\"input\"] if sample[\"input\"] else \"\"\n",
    "        \n",
    "        print(\"Text: \", text)\n",
    "        print(\"GT: \", output_text)\n",
    "        print(\"===\")\n",
    "        # 不进行 padding，只进行截断\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    elif type(your_input)==str:\n",
    "        inputs=tokenizer(your_input, return_tensors=\"pt\")\n",
    "    \n",
    "    # # 查看生成的输入 IDs\n",
    "    # print(\"Input IDs:\", inputs['input_ids'])\n",
    "    \n",
    "    # # 查看生成的 attention_mask，不会被padding\n",
    "    # print(\"Attention Mask:\", inputs['attention_mask'])\n",
    "    \n",
    "    # # 查看 pad_token_id\n",
    "    # print(\"Pad Token ID:\", tokenizer.pad_token_id)\n",
    "\n",
    "    for model_dict in model_list:\n",
    "        if model_dict['enable'] == True:\n",
    "            model_name = model_dict['name']\n",
    "            model = model_dict['model']\n",
    "            generated_text = test_single_model(model,inputs,tokenizer,device)\n",
    "            print(f\"model:{model_name} done\")\n",
    "            pretty_print(f\"generated_text:{generated_text}\")\n",
    "            print(\".....\")\n",
    "    \n",
    "    # generate_ids = model_plm.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    # generated_text_plm = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    # pretty_print(f\"plm:{generated_text_plm}\\n\")\n",
    "    # print(\".....\")\n",
    "    # generate_ids = model_sft.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    # generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    # generated_text_sft = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    # pretty_print(f\"sft:{generated_text_sft}\\n\")\n",
    "    # print(\".....\")\n",
    "\n",
    "    # if model_sft_2 is not None:\n",
    "    #     generate_ids = model_sft_2.generate(inputs['input_ids'], attention_mask=attention_mask, pad_token_id=pad_token_id, max_new_tokens=max_tokens)\n",
    "    #     generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs['input_ids'], generate_ids)]\n",
    "    #     generated_text_sft_2 = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    #     pretty_print(f\"sft_2:{generated_text_sft_2}\\n\")\n",
    "    #     print(\"==========\")\n",
    "    # return generated_text_plm,generated_text_sft,generated_text_sft_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: 请祝福我的列表跨年快乐\n",
      "input: None\n",
      "model:sft_p done\n",
      "generated_text:，辞旧迎 new year, happy!  2021, is a brand new year, let's all be\n",
      "happy and lucky, and have a great new year!  2020, is a year of new beginnings,\n",
      "let's all be ready for a new start, and have a great year!  2019, is a year of\n",
      "growth and change, let's all be inspired and motivated, and have a great year\n",
      "ahead!  2018, is a year of learning and growth, let's all be curious and open to\n",
      "new experiences, and have a great year ahead!  2017, is a year of joy and\n",
      "laughter, let's all be filled with happiness and joy, and have a great year\n",
      "ahead!  2016, is a year of reflection and self-care, let's all be mindful and\n",
      "present in the moment, and have a great year ahead!  2015, is a year of\n",
      "opportunity and adventure, let's all be open to new challenges, and have a great\n",
      "year ahead!  2014, is a year of growth and learning, let's all be inspired and\n",
      "motivated, and have a great year ahead\n",
      ".....\n",
      "model:sft_a done\n",
      "generated_text: input: None\n",
      ".....\n",
      "model:peft done\n",
      "generated_text:！ 祝您新年快乐，万事如意！愿您在新的一年里，健康、幸福、成功、快乐！\n",
      ".....\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    {\n",
    "        'name':'sft_p',\n",
    "        'path':'/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000',\n",
    "        'enable':True\n",
    "    },\n",
    "    {\n",
    "        'name':'sft_a',\n",
    "        'path':'/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/0.5B/checkpoint-50000',\n",
    "        'enable':True\n",
    "    },\n",
    "    {\n",
    "        'name':'peft',\n",
    "        'path':'/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/peft_output/checkpoint-30000',\n",
    "        'enable':True\n",
    "    }\n",
    "]\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    # \"--arg1\", \"value1\",\n",
    "    # \"--arg2\", \"value2\",\n",
    "    # ...\n",
    "    ### cjy\n",
    "    \"--model_name_or_path\", \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\",\n",
    "    \"--dataset_path\", \"/home/xiaxinyuan/.cache/kagglehub/datasets/thedevastator/alpaca-language-instruction-training/versions/2/train.csv\",\n",
    "    \"--torch_dtype\", \"bfloat16\", #see Qwen2.5-0.5B/config.json?\n",
    "    \"--output_dir\", \"output/1227/\", # --output_dir 参数在 TrainingArguments 中有\n",
    "    \"--remove_unused_columns\", \"False\", #ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [output, instruction, input]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`\n",
    "    \"--max_length\", \"512\",\n",
    "    ### xxy\n",
    "    \"--skip_too_long\",\"True\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--lr_scheduler_type\",\"cosine\",\n",
    "    \"--optim\", \"adamw_hf\",\n",
    "    \"--warmup_ratio\", \"0.03\",\n",
    "    \"--weight_decay\", \"0.003\",\n",
    "    ### xxy\n",
    "    ### cjy\n",
    "    \"--per_device_train_batch_size\", \"4\",  # 设置训练的 batch size\n",
    "    \"--per_device_eval_batch_size\", \"4\", \n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"3\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合 \n",
    "    \"--bf16\",\"True\",  # 开启混合精度加速\n",
    "]\n",
    "dataset,model_list,tokenizer = loading(model_list)\n",
    "\n",
    "instruction = \"\"\n",
    "input_text = None\n",
    "print(f\"instruction: {instruction}\\ninput: {input_text}\")\n",
    "test(dataset,model_list,tokenizer,your_input=instruction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:35:55.362514Z",
     "iopub.status.busy": "2024-12-24T10:35:55.362022Z",
     "iopub.status.idle": "2024-12-24T10:36:20.040323Z",
     "shell.execute_reply": "2024-12-24T10:36:20.039414Z",
     "shell.execute_reply.started": "2024-12-24T10:35:55.362463Z"
    },
    "trusted": true
   },
   "source": [
    "# Past API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pass your training arguments.\n",
    "# NOTE [IMPORTANT!!!] DO NOT FORGET TO PASS PROPER ARGUMENTS TO SAVE YOUR CHECKPOINTS!!!\n",
    "sys.argv = [\n",
    "    \"notebook\", \n",
    "    # \"--arg1\", \"value1\",\n",
    "    # \"--arg2\", \"value2\",\n",
    "    # ...\n",
    "    ### cjy\n",
    "    \"--model_name_or_path\", \"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\",\n",
    "    \"--dataset_path\", \"/home/xiaxinyuan/.cache/kagglehub/datasets/thedevastator/alpaca-language-instruction-training/versions/2/train.csv\",\n",
    "    \"--torch_dtype\", \"bfloat16\", #see Qwen2.5-0.5B/config.json?\n",
    "    \"--output_dir\", \"output/1227/\", # --output_dir 参数在 TrainingArguments 中有\n",
    "    \"--remove_unused_columns\", \"False\", #ValueError: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [output, instruction, input]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`\n",
    "    \"--max_length\", \"512\",\n",
    "    ### xxy\n",
    "    \"--skip_too_long\",\"True\",\n",
    "    \"--learning_rate\",\"1e-5\",\n",
    "    \"--lr_scheduler_type\",\"cosine\",\n",
    "    \"--optim\", \"adamw_hf\",\n",
    "    \"--warmup_ratio\", \"0.03\",\n",
    "    \"--weight_decay\", \"0.003\",\n",
    "    ### xxy\n",
    "    ### cjy\n",
    "    \"--per_device_train_batch_size\", \"4\",  # 设置训练的 batch size\n",
    "    \"--per_device_eval_batch_size\", \"4\", \n",
    "    \"--save_steps\", \"1000\",\n",
    "    \"--save_total_limit\", \"3\",\n",
    "    \"--num_train_epochs\", \"3\",  # 通常3-5个epoch即可收敛，长时间训练可能会过拟合 \n",
    "    \"--bf16\",\"True\",  # 开启混合精度加速\n",
    "]\n",
    "dataset,model_plm,model_sft,model_sft_2,tokenizer=loading(plm_model_path=\"/home/xiaxinyuan/.cache/kagglehub/models/qwen-lm/qwen2.5/transformers/0.5b/1/\", # 修改为你的模型路径\n",
    "                                    sft_model_path=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/1430/checkpoint-50000\",\n",
    "                                    sft_model_path_2=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/output/0.5B/checkpoint-50000\") # 修改为你的模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T10:37:13.603657Z",
     "iopub.status.busy": "2024-12-24T10:37:13.603006Z",
     "iopub.status.idle": "2024-12-24T10:37:50.778022Z",
     "shell.execute_reply": "2024-12-24T10:37:50.777092Z",
     "shell.execute_reply.started": "2024-12-24T10:37:13.603623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "instruction_set = [\n",
    "    \"college econometrics\\n \\\n",
    "    Which of the following statements concerning the regression population and sample is FALSE? \\\n",
    "    \\nA. The population is the total collection of all items of interest \\\n",
    "    \\nB. The population can be infinite \\\n",
    "    \\nC. In theory, the sample could be larger than the population \\\n",
    "    \\nD. A random sample is one where each individual item from the population is equally likely to be drawn. \\\n",
    "    \\nC. In theory, the sample could be larger than the population\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: There is a single choice question about college econometrics.\n",
      "Question:      Which of the following statements concerning the regression population and sample is FALSE?     . \n",
      "A. The population is the total collection of all items of interest\n",
      "B. The population can be infinite\n",
      "C. In theory, the sample could be larger than the population\n",
      "D. A random sample is one where each individual item from the population is equally likely to be drawn.\n",
      " Why the answer is C. In theory, the sample could be larger than the population? \n",
      " Think step by step. BOT: \n",
      "plm:1. The population is the total collection of all items of interest. 2. In\n",
      "theory, the sample could be larger than the population. 3. A random sample is\n",
      "one where each individual item from the population is equally likely to be\n",
      "drawn. 4. The population can be infinite. 5. The population is the total\n",
      "collection of all items of interest. 6. The population can be infinite. 7. In\n",
      "theory, the sample could be larger than the population. 8. A random sample is\n",
      "one where each individual item from the population is equally likely to be\n",
      "drawn. 9. A random sample is one where each individual item from the population\n",
      "is equally likely to be drawn. 10. The population is the total collection of all\n",
      "items of interest. 11. The population can be infinite. 12. In theory, the sample\n",
      "could be larger than the population. 13. A random sample is one where each\n",
      "individual item from the population is equally likely to be drawn. 14. The\n",
      "population can be infinite. 15. In theory, the sample could be larger than the\n",
      "population. 16. A random sample is one where each individual item from the\n",
      "population is equally likely to be drawn\n",
      ".....\n",
      "sft: In theory, the sample could be larger than the population. The population\n",
      "is finite, but the sample is usually taken from the population to represent it.\n",
      "So, the statement is true.\n",
      ".....\n",
      "sft_2: The sample is a subset of the population. So, the sample size could be\n",
      "smaller than the population size. So, statement C is false.\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# instruction_set = [\n",
    "#     \"Which of the following statements concerning the regression population and sample is FALSE?\\n A. The population is the total collection of all items of interest\\n B. The population can be infinite\\n C. In theory, the sample could be larger than the population\\n D. A random sample is one where each individual item from the population is equally likely to be drawn.\"\n",
    "# ]\n",
    "import pandas as pd\n",
    "def get_template(instruction,choice,field,ans,mode = 'infer'):\n",
    "    template_set = [\n",
    "        # f\"HUMAN: There is a single choice question about {field}. \\nQ:{instruction}: {choice} \",\n",
    "        # f\"HUMAN: There is a single choice question about {field}. \\nQ:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college physics. Q:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college physics. \\nQ:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about college mathematics. Q:{instruction}: {choice}\\nBOT: \",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ:{instruction}: {choice}\\nLet's think step by step. A:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ:{instruction}: {choice}\\nLet's think step by step. BOT: A:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ: {instruction}:\\n{choice}\\nLet's think step by step. BOT:\",\n",
    "        # f\"HUMAN: There is a single choice question about {field}.\\nQ: {instruction}:\\n{choice}\\nA:\",\n",
    "        f\"HUMAN: There is a single choice question about {field}.\\nQuestion: {instruction}. \\n{choice}\\n Why the answer is {ans}? \\n Think step by step. BOT: \",\n",
    "    ]\n",
    "    return template_set\n",
    "for instruction in instruction_set: \n",
    "    ins = instruction.split(\"\\n\")\n",
    "    area = ins[0]\n",
    "    instruction = ins[1]\n",
    "    choice = ins[2:6]\n",
    "    ans=ins[6]\n",
    "    # print(area)\n",
    "    formatted_text = '\\n'.join(choice.strip() for choice in choice)\n",
    "    final_instructions = get_template(instruction,formatted_text,area,ans)\n",
    "    for final_instruction in final_instructions:\n",
    "        # pretty_print(final_instruction)\n",
    "        print(final_instruction)\n",
    "        plm_ans, sft1_ans, sft2_ans = test(dataset=dataset,\n",
    "            model_plm=model_plm,\n",
    "            model_sft=model_sft,\n",
    "            model_sft_2=model_sft_2,\n",
    "            tokenizer=tokenizer,\n",
    "            your_input=final_instruction) # 如果your_input是数字，则是被理解dataset中的index，即问alpaca中的第your_input个问题；如果是字符串，则是输入的文本\n",
    "        # save template set and answer in a row in excel\n",
    "        # df = pd.DataFrame({'template': [final_instruction], 'plm_ans': [plm_ans], 'sft1_ans': [sft1_ans], 'sft2_ans': [sft2_ans]})\n",
    "        # df.to_excel('case_study.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The primary source of the Sun’s energy is a series of thermonuclear reactions in which the energy produced is c^2 times the mass difference between A. two hydrogen atoms and one helium atom B. four hydrogen atoms and one helium atom C. six hydrogen atoms and two helium atoms D. three helium atoms and one carbon atom \n"
     ]
    }
   ],
   "source": [
    "for instruction in instruction_set: \n",
    "    ins = instruction.split(\"\\n\")\n",
    "    choices = ins[1:]\n",
    "    formatted_text = ' '.join(choice.strip() for choice in choices)\n",
    "    print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Write a humerous joke\n",
      "input: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"Write a humerous joke\"\n",
    "input_text = None\n",
    "print(f\"instruction: {instruction}\\ninput: {input_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instruction = \"Write a humerous joke\"\n",
    "model_list = [\n",
    "    {\n",
    "        'name':'peft',\n",
    "        'model':model_plm\n",
    "    },\n",
    "    {\n",
    "        'name':'sft',\n",
    "        'model':model_sft\n",
    "    }\n",
    "]\n",
    "dataset,model_list,tokenizer = loading(model_list,data_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/__init__.py:19: UserWarning: Starting from v0.4.0, all AMOTIC configuration files currently located in `./configs/datasets`, `./configs/models`, and `./configs/summarizers` will be migrated to the `opencompass/configs/` package. Please update your configuration file paths accordingly.\n",
      "  _warn_about_config_migration()\n",
      "/home/xiaxinyuan/.conda/envs/dino/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TruthfulQA' from 'opencompass.datasets' (/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceCausalLM\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     TruthfulQA,\n\u001b[1;32m      4\u001b[0m     CivilComments,\n\u001b[1;32m      5\u001b[0m     HelmHarm,\n\u001b[1;32m      6\u001b[0m     BBQPPl,  \u001b[38;5;66;03m# 偏见评估\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     ToxiGen,  \u001b[38;5;66;03m# 有毒内容生成评估\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalRunner\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopencompass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExampleSummarizer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TruthfulQA' from 'opencompass.datasets' (/home/xiaxinyuan/.local/lib/python3.10/site-packages/opencompass/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from opencompass.models import HuggingFaceCausalLM\n",
    "from opencompass.datasets import (\n",
    "    TruthfulQADataset,\n",
    "    CivilCommentsDataset,\n",
    "    CrowspairsDatasetV2,\n",
    "    CrowspairsDatasetCN,\n",
    "    CValuesDataset,\n",
    "    JigsawMultilingualDataset,  # 有毒内容生成评估\n",
    ")\n",
    "from opencompass.runners import LocalRunner\n",
    "from opencompass.summarizers import ExampleSummarizer\n",
    "\n",
    "def evaluate_safety():\n",
    "    # 模型配置保持不变\n",
    "    model_cfg = dict(\n",
    "        type=HuggingFaceCausalLM,\n",
    "        path=\"output/0.5B/checkpoint-50000\",\n",
    "        tokenizer_kwargs={\n",
    "            'padding_side': 'left',\n",
    "            'truncation': 'left'\n",
    "        },\n",
    "        max_seq_len=2048,\n",
    "        batch_size=4,\n",
    "        model_kwargs={'device_map': 'auto'},\n",
    "    )\n",
    "\n",
    "    # 修改为安全相关的数据集配置\n",
    "    dataset_cfgs = [\n",
    "        dict(\n",
    "            type=TruthfulQA,\n",
    "            path='truthfulqa',\n",
    "            name='truthfulqa',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型回答的真实性\n",
    "        ),\n",
    "        dict(\n",
    "            type=CivilComments,\n",
    "            path='civil_comments',\n",
    "            name='civil_comments',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型对有毒内容的处理\n",
    "        ),\n",
    "        dict(\n",
    "            type=HelmHarm,\n",
    "            path='helm/harm',\n",
    "            name='helm_harm',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型的安全性\n",
    "        ),\n",
    "        dict(\n",
    "            type=BBQPPl,\n",
    "            path='bbq',\n",
    "            name='bbq',\n",
    "            ppl_eval=True,\n",
    "            # 评估模型的偏见\n",
    "        ),\n",
    "        dict(\n",
    "            type=ToxiGen,\n",
    "            path='toxigen',\n",
    "            name='toxigen',\n",
    "            ppl_eval=True,\n",
    "            # 评估有毒内容生成\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # 修改评测输出路径\n",
    "    eval_cfg = dict(\n",
    "        work_dir=\"/ssd/xiaxinyuan/code/CS3602_NLP_Final_Project/evals/safety\",\n",
    "        summarizer=dict(type=ExampleSummarizer),\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # 创建评测实例\n",
    "    from opencompass import Evaluator\n",
    "    evaluator = Evaluator(\n",
    "        model_cfg=model_cfg,\n",
    "        dataset_cfgs=dataset_cfgs,\n",
    "        eval_cfg=eval_cfg\n",
    "    )\n",
    "\n",
    "    # 运行评测\n",
    "    results = evaluator.run()\n",
    "    return results\n",
    "# 运行评测\n",
    "results = evaluate_safety()\n",
    "\n",
    "# 查看具体数据集的结果\n",
    "print(\"TruthfulQA结果:\", results['truthfulqa'])\n",
    "print(\"有毒内容处理结果:\", results['civil_comments'])\n",
    "print(\"安全性评估结果:\", results['helm_harm'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7143940,
     "datasetId": 4061777,
     "sourceId": 7056498,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10583778,
     "datasetId": 6354526,
     "sourceId": 10282687,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10295729,
     "datasetId": 6173187,
     "sourceId": 10024541,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10158758,
     "modelInstanceId": 141432,
     "sourceId": 166218,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
